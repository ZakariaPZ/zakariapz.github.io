---
title: "Derivative of the Cross Entropy Loss With Softmax"
date: 2025-03-19
output:
  distill::distill_article:
    self_contained: false
---


To compute the derivative $\frac{\partial L}{\partial a_i}$, we first rewrite the derivative using the chain rule,

$$
  \frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial p_j} \frac{\partial p_j}{\partial a_i}.
$$
Here, we've used $p_j$ to denote the $j$th entry of the Softmax output, which is a vector of $C$ elements (one per class),

$$
  p_j = \frac{\exp( a_j)}{\sum_k^C \exp(a_k)}.
$$
Starting with the first term in the chain rule,

$$
  \frac{\partial L}{\partial p_j} = \frac{\partial}{\partial p_j} \left( -\sum_i^C y_i \log p_i \right).
$$ 
Assuming the correct label is $y_c$, all other elements $y_{i\neq c}$ are zero, so we obtain the following expression:

$$
  \frac{\partial L}{\partial p_j} = -\frac{\partial}{\partial p_j} \left(y_c \log p_c \right). 
$$ 
If $j\neq c$, the contribution to the gradient is zero because $y_c=0$. For a training example $x_n$ where the true class is $c$, only the Softmax probability $p_c$ contributes to the loss as only $y_c=1$ and all other $p_{j\neq c}$ are removed as $y_{j \neq c} = 0$.

Otherwise, when $j = c$,
$$
  \frac{\partial L}{\partial p_c} = -\frac{\partial}{\partial p_c} \left(y_c \log p_c \right) \\
  = -\frac{y_c}{p_c}.
$$ 
Next, we compute the second term of the chain rule,

$$
  \frac{\partial p_j}{\partial a_i} = \frac{\partial }{\partial a_i} \frac{\exp( a_j)}{\sum_k^C \exp(a_k)}
$$
If $i=j$, 

$$
  = \frac{\partial }{\partial a_j} \frac{\exp( a_j)}{\sum_k^C \exp(a_k)} \\
  = \frac{\partial }{\partial a_j} \exp( a_j) \left(\sum_k^C \exp(a_k) \right)^{-1} \\
  = \exp( a_j) \left(\sum_k^C \exp(a_k) \right)^{-1} - 
    \exp( a_j) \left(\sum_k^C \exp(a_k) \right)^{-2} \exp( a_j) \\
  = p_j -p_j^2 \\
  = p_j(1-p_j).
$$
And if $i \neq j$,

$$
  = \frac{\partial }{\partial a_i} \frac{\exp( a_j)}{\sum_k^C \exp(a_k)} \\
  = \exp( a_j) \frac{\partial }{\partial a_i}\left(\sum_k^C \exp(a_k) \right)^{-1} \\
  = -\exp( a_j) \left(\sum_k^C \exp(a_k) \right)^{-2} \exp(a_i) \\
  = -p_jp_i. \\
$$

Both of these cases can be combined,
$$
  \frac{\partial p_j}{\partial a_i} = p_j(\delta_{ij} -p_i).
$$

However, we know from before that terms with $j \neq c$ are zero. So, we can set $j=c$ and substitute the expressions for each term in the chain rule to get the final derivative,

$$
  \frac{\partial L}{\partial a_i} = \frac{\partial L}{\partial p_j} \frac{\partial p_j}{\partial a_i} \\
   = -\frac{y_c}{p_c} p_c(\delta_{ic} - p_i) \\
   = p_i - y_c \delta_{ic}.
$$
The gradient is $p_i$ if $i \neq c$, and $p_c-y_c$ if $i=c$. Since $y$ is a one-hot vector with $y_{i=c}=1$ and $y_{i\neq c}=0$, we can simply write, 

$$
  \frac{\partial L}{\partial a_i} = p_i - y_i.
$$



[
  {
    "path": "posts/2025-02-27-backpropagation/",
    "title": "Backpropagation",
    "description": "A short description of the post.",
    "author": [],
    "date": "2025-02-27",
    "categories": [],
    "contents": "\r\n← Back\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2025-02-27T20:32:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Decoder-Only Transformers",
    "description": "A look into the architecture that powers most LLMs.",
    "author": [],
    "date": "2025-02-26",
    "categories": [],
    "contents": "\r\n← Back\r\nSelf-Attention\r\nAt the core of the transformer is the attention mechanism. For each input token \\(x_n\\), the attention mechanism aggregates information from surrounding tokens and produces a new representation \\(y_n\\). This is known as self-attention, because the input sequence attends to itself to create a new representation of the same sequence. The simplest way to compute this is to take a linear combination of all the input tokens,\r\n\\[\r\n  y_n=\\sum_{m}^N a_{nm} x_m\r\n\\]\r\nA token \\(x_m\\) that is closely related \\(x_n\\) should contribute a lot to the computation of \\(y_n\\). Conversely, tokens that have very little to do with \\(x_n\\) are unlikely to provide any interesting context. Intuitively then, \\(a_{nm}\\) should capture how similar the tokens \\(x_n\\) and \\(x_m\\) are. Tokens which are closely related should have a large \\(a_{nm}\\), while unrelated tokens should contribute close to nothing, i.e., \\(a_{nm} \\approx 0\\). This is easy to do with vectors - we can simply compute their dot product and normalize them so that no weight is negative. We also make sure they sum to a constant so they act as weights. The Softmax function satisfies both criteria,\r\n\\[\r\n  a_{nm} = \\frac{\\exp\\left(x_n^T x_m\\right)}{\\sum_m^N{\\exp\\left(x_n^T x_m\\right)}}.\r\n\\]\r\nThen, the final representation for \\(x_n\\) is computed as follows:\r\n\\[\r\n  y_n=\\sum_{m}^N a_{nm} x_m, \\quad \\text{where} \\ \\ a_{nm}\\geq0 \\ \\ \\text{and} \\ \\ \\sum a_{nm} = 1.\r\n\\]\r\nSo far, this attention mechanism has no learning capacity. We have to this point this far simply by reasoning how each input token can be better represented according to its similarity to other input tokens. However, the original representations \\(X\\) may not be optimal for determining the nuanced relationships between different tokens. We can afford this mechanism some additional flexibility by introducing a linear transformation \\(Q\\) that will hopefully transform each token such that their dot product reveals more interesting relationships between them,\r\n\\[\r\n  a_{nm} = \\frac{\\exp\\left((Qx_n)^T Qx_m \\right)}{\\sum_m^N{\\exp\\left((Qx_n)^T Qx_m\\right)}} \\\\\r\n  = \\frac{\\exp\\left(x_n^T Q^TQx_m \\right)}{\\sum_m^N{\\exp\\left(x_n^T Q^TQ x_m\\right)}}\r\n\\]\r\nThe entries of such a matrix are obviously unknown - they should be parameters learned from the data. But there’s a subtle problem with using the same matrix \\(Q\\) to transform both \\(x_n\\) and \\(x_m\\).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2025-02-28T02:30:38-05:00",
    "input_file": {}
  }
]
